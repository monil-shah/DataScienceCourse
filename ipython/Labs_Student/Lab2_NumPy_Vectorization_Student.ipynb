{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate a random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of columns (features)\n",
    "K = 5\n",
    "\n",
    "#Number of records\n",
    "N = 1000\n",
    "\n",
    "#Generate an NxK matrix of uniform random variables\n",
    "X = np.random.random([N,K])#Student: generate a uniform random matrix here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peak at our data to confirm it looks as we expect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72522322,  0.25052096,  0.19574015,  0.55063655,  0.99120074])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student - Put in a command to view the first 100 rows\n",
    "X[100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student - put in a command to see the dimensions of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about designing a scoring function for a logistic regression. As we are not concerned with fitting a model to data, we can just make up a logistic regression. <br> <br>\n",
    "\n",
    "For quick intro, the Logistic Regression takes the form of $\\hat{Y} = f(x * \\beta^T)$, where $x$ is the $1xK$ vector of features and $\\beta$ is the $1xK$ vector of weights. The function $f$, called a 'link' function, is the inverse logit: <br><br>\n",
    "\n",
    "<center>$f(a)=\\frac{1}{1+e^{-a}}$</center> <br><br>\n",
    "\n",
    "In this notebook we'll write a function that, given inputs of $X$ and $\\beta$, returns a value for $\\hat{Y}$.\n",
    "<br><br>\n",
    "First let's generate a random set of weights to represent $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.74974785,  0.84953288,  0.73985974, -0.98112893,  0.6732545 ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Student - generate a K dimensional vector of uniform random variables in the interval [-1, 1]\n",
    "#beta = np.random.uniform(-1,1,K)#input command here\n",
    "beta = 2*(np.random.random(K)-0.5)  #Vectorization\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we applied a neat NumPy trick here. The numpy.random.random() function returns an array, yet we applied what appears to be a scalar operation on the vector. This is an example of what NumPy calls vectorization (a major point of this tutorial), which offers us both a very fast way to do run vector computations as well as a clean and concise method of coding. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "<b>Question: we designed the above $beta$ vector such that $E[\\beta_i]=0$. How can we confirm that we did this correctly?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40625320725531433"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start by taking the mean of the beta we already calculated\n",
    "\n",
    "#Student - fill in command here\n",
    "#np.mean(beta)\n",
    "beta.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It is likely the above is not equal to zero. Let's simulate this 100k times and see what the distribution of means is\n",
    "#Student input code here\n",
    "means = []\n",
    "for i in range(0,100000):\n",
    "    beta = 2*(np.random.random(K)-0.5)  \n",
    "    means.append(beta.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use matplotlibs hist function to plot the histogram of means here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba2b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect the distribution to be centered around zero. Is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write our scoring function. Let's try to use as much of Numpy's inner optimization as possible (hint, this can be done in two lines and without writing any loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_logistic_regression(X, beta):\n",
    "    '''\n",
    "    This function takes in an NxK matrix X and 1xK vector beta.\n",
    "    The function should apply the logistic scoring function to each record of X.\n",
    "    The output should be an Nx1 vector of scores\n",
    "    '''\n",
    "    \n",
    "    #First let's calculate X*beta - make sure to use numpy's 'dot' method\n",
    "    #student - put in code here\n",
    "    a=X.dot(beta)\n",
    "    #Now let's input this into the link function\n",
    "    #student - put in code here\n",
    "    \n",
    "    prob_score= (1/(1+np.exp(-1*a)))\n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how much faster is it by using Numpy? We can test this be writing the same function that uses no Numpy and executes via loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_logistic_regression_NoNumpy(X, beta):\n",
    "    '''\n",
    "    This function takes in an NxK matrix X and 1xK vector beta.\n",
    "    The function should apply the logistic scoring function to each record of X.\n",
    "    The output should be an Nx1 vector of scores\n",
    "    '''\n",
    "    #Let's calculate xbeta using loops\n",
    "    xbeta = []\n",
    "    for row in X:\n",
    "        \n",
    "        xb = 0\n",
    "        for i, el in enumerate(row):\n",
    "            xb += el * beta[i]\n",
    "        \n",
    "        xbeta.append(xb)\n",
    "        \n",
    "    #Now let's apply the link function to each xbeta\n",
    "    prob_score = []\n",
    "    for xb in xbeta:\n",
    "        prob_score.append(1 / (1 + np.exp(-1 * xb)))\n",
    "        \n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any analysis, let's test the output of each to make sure they equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 0.0\n"
     ]
    }
   ],
   "source": [
    "#Student - write a unit test that calls each function with the same inputs and checks to see they return the same values.\n",
    "diff = np.abs(score_logistic_regression(X, beta)-score_logistic_regression_NoNumpy(X, beta))\n",
    "print('Mean = {}'.format(np.round(diff.sum(),0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If they equal then we can proceed with timing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.67 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit score_logistic_regression_NoNumpy(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.39 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 17.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit score_logistic_regression(X, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
